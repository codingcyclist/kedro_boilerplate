{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3e291a3",
   "metadata": {},
   "source": [
    "# Example for using `pyspark` in `kedro`\n",
    "\n",
    "This notebook demonstrates how to leverage `spark` capabilities within the `kedro` framework. The huge benefit of `spark` lies in its **distributed, lazy loading paradigm**: First of all, `spark` can process huge amounts of data without loading the entire data into memory and second, scaling `spark` pipelines vertically accross a multitude of nodes in a compute cluster is super easy.\n",
    "\n",
    "Below, you'll find examples for how to...\n",
    "1. Load a `spark` session and use it to convert a pandas DataFrame into a spark DataFrame\n",
    "2. Add a `SparkDataSet` to the kedro catalog\n",
    "3. Save and load a `spark` DataFrame to/from the kedro catalog\n",
    "4. Use `pyspark` to transform and analyze data\n",
    "\n",
    "---\n",
    "\n",
    "## Load a `spark` session and convert a pandas- into a `spark` DataFrame\n",
    "One of spark's core principles is that there is always onle one spark session. In our case, this spark session has already been configured and created when we launched this kedro notebook (take a look at `src/context.py` to convince yourself).\n",
    "Hence we can just use `SparkSession.builder.getOrCreate()` to piggy-back on that pre-configured spark session.\n",
    "\n",
    "The data which we convert from pandas into a spark DataFrame is just a publickly available retail data set from the UCI machine learning repository. In the end, we'll use spark to analyze which customers generated the most revenue.\n",
    "Also, we provide a **schema** when we transform the data into a spark DataFrame. By providing a schema, we drastically **reduce the memory footprint** of the underlying computation as we avoid that spark has to scan the entire data up-front to infer the data types itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23c2d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "RETAIL_DATA_SCHEMA = 'InvoiceNo STRING, StockCode STRING, Description STRING, Quantity INT, InvoiceDate TIMESTAMP, '+\\\n",
    "    'UnitPrice FLOAT, CustomerID INT, Country STRING'\n",
    "\n",
    "retail_data = pd.read_excel(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx',\n",
    ")\n",
    "retail_data_spark = spark.createDataFrame(\n",
    "    data=retail_data.assign(CustomerID=retail_data['CustomerID'].fillna(0).astype(int)),\n",
    "    schema=RETAIL_DATA_SCHEMA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103a2716",
   "metadata": {},
   "source": [
    "## Add a `SparkDataSet` to the kedro catalog\n",
    "\n",
    "For demo purposes, we're not using `context.catalog`, but create our own, local `catalog` object. However, the following steps (reading, writing, analyzing) would have worked in exactly the same way if `retail_data_spark` was defined in `catalog.yml`\n",
    "\n",
    "Important to note here are the `load_args` and `save_args`, which we provide to the `SparkDataSet`. In particular, it is crucial for performance reasons to provide a schema via `load_args`. Moreover, make sure to set `mode` to `overwrite` when your pipelines whrite to the same data set several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e5dee28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kedro/.local/lib/python3.7/site-packages/fsspec/__init__.py:43: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for spec in entry_points.get(\"fsspec.specs\", []):\n"
     ]
    }
   ],
   "source": [
    "# create a DataCatalog and add a SparkDataset\n",
    "from kedro.io import DataCatalog\n",
    "from kedro.extras.datasets.spark import SparkDataSet\n",
    "import os\n",
    "\n",
    "RETAIL_DATA_SCHEMA = 'InvoiceNo STRING, StockCode STRING, Description STRING, Quantity INT, InvoiceDate TIMESTAMP, '+\\\n",
    "    'UnitPrice FLOAT, CustomerID INT, Country STRING'\n",
    "\n",
    "catalog = DataCatalog({'retail_data_spark':SparkDataSet(\n",
    "        filepath=os.path.join(context.project_path,'data','01_raw','retail_data'),\n",
    "        file_format='csv',\n",
    "        load_args={'header':True, 'schema':RETAIL_DATA_SCHEMA},\n",
    "        save_args={'header':True, 'mode':'overwrite'}\n",
    ")})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd9776",
   "metadata": {},
   "source": [
    "## Save and load a spark DataFrame to/from the kedro catalog\n",
    "\n",
    "Once the `SparkDataSet` is configured, we can read/write to/from the data set just as we'd do with any other kedro data set. The only difference is that since we're using `spark`, we don't get the data as a pandas, but as a `spark` DataFrame (including all the useful lazy-loading and scalability benefits of spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8841ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:12:45,895 - kedro.io.data_catalog - INFO - Saving data to `retail_data_spark` (SparkDataSet)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kedro/.local/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "catalog.save('retail_data_spark', retail_data_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bdffa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:12:54,974 - kedro.io.data_catalog - INFO - Loading data from `retail_data_spark` (SparkDataSet)...\n"
     ]
    }
   ],
   "source": [
    "retail_data_spark_loaded = catalog.load('retail_data_spark')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7d0b2",
   "metadata": {},
   "source": [
    "## Use `pyspark` to transform and analyze data\n",
    "\n",
    "The section below is just to showcase how to perform some typical data analysis tasks in pyspark (e.g. data inspection & aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "725771f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: float (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspecting the data\n",
    "retail_data_spark_loaded.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cb7c1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "revenue_per_customer = retail_data_spark_loaded.\\\n",
    "    withColumn(\"invoice_amount\", retail_data_spark_loaded['Quantity'] * retail_data_spark_loaded.UnitPrice).\\\n",
    "    groupBy(retail_data_spark_loaded.CustomerID).\\\n",
    "    agg(f.sum(f.col(\"invoice_amount\")).alias(\"invoice_amount\")).\\\n",
    "    orderBy(f.col(\"invoice_amount\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a172cdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|    invoice_amount|\n",
      "+----------+------------------+\n",
      "|         0| 1447682.118258085|\n",
      "|     14646|279489.01930066943|\n",
      "|     18102|256438.48995232582|\n",
      "|     17450| 187482.1704902649|\n",
      "|     14911|132572.61962211132|\n",
      "|     12415|123725.44990730286|\n",
      "|     14156|113384.13939154148|\n",
      "|     17511| 88125.37999302149|\n",
      "|     16684| 65892.07912826538|\n",
      "|     13694| 62653.09981799126|\n",
      "|     15311| 59419.33947509527|\n",
      "|     13089| 57385.88000047207|\n",
      "|     14096| 57120.91003343463|\n",
      "|     15061|54228.739621818066|\n",
      "|     17949|52750.839977264404|\n",
      "|     15769| 51823.72010803223|\n",
      "|     16029| 50992.60983848572|\n",
      "|     14298|  50862.4400164485|\n",
      "|     14088| 50415.48962235451|\n",
      "|     17841| 40340.77974051237|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "revenue_per_customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b4f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
